# Issue #3: isa_model åŒ…çš„å¯¼å…¥ç»“æ„é—®é¢˜ - ä¸å¿…è¦çš„ä¾èµ–æ±¡æŸ“

## é—®é¢˜æè¿°

`isa_model[cloud]` extras æœ¬åº”æ˜¯è½»é‡çº§çš„äº‘ API æ¨¡å¼ä¾èµ–ï¼Œä½†ç”±äºä¸åˆç†çš„å¯¼å…¥ç»“æ„ï¼Œè¢«è¿«å¼•å…¥äº†æœ¬åº”åªåœ¨ `[local]` æ¨¡å¼éœ€è¦çš„ä¾èµ–ï¼š
- `huggingface-hub` (åŸæœ¬åœ¨ `[local]`)
- `docker` (ç”¨äº Triton éƒ¨ç½²)

è¿™å¯¼è‡´ Docker é•œåƒä½“ç§¯å¢å¤§ï¼Œä¾èµ–ç®¡ç†æ··ä¹±ã€‚

## æ ¹æœ¬åŸå› 

### å¯¼å…¥é“¾åˆ†æ

**é—®é¢˜å¯¼å…¥é“¾**:
```
isa_model/__init__.py
  â””â”€> client.py
      â””â”€> inference/ai_factory.py
          â””â”€> inference/services/__init__.py
              â””â”€> inference/services/llm/__init__.py:10
                  â””â”€> LocalLLMService  âŒ æ— æ¡ä»¶å¯¼å…¥
                      â””â”€> deployment/local/__init__.py
                          â””â”€> deployment/__init__.py:7
                              â””â”€> deployment/modal/deployer.py:16
                                  â””â”€> huggingface_hub  âŒ å¼ºåˆ¶ä¾èµ–
                              â””â”€> deployment/triton/provider.py:16
                                  â””â”€> docker  âŒ å¼ºåˆ¶ä¾èµ–
```

### é—®é¢˜æ–‡ä»¶

**1. `isa_model/inference/services/llm/__init__.py`** (ç¬¬ 10 è¡Œ)
```python
# âŒ æ— æ¡ä»¶å¯¼å…¥ LocalLLMService
from .local_llm_service import LocalLLMService, create_local_llm_service
```

**é—®é¢˜**: `LocalLLMService` åªåœ¨æœ¬åœ°æ¨¡å‹æ¨ç†æ—¶éœ€è¦ï¼Œcloud æ¨¡å¼æ ¹æœ¬ä¸åº”è¯¥å¯¼å…¥ã€‚

**2. `isa_model/deployment/__init__.py`** (ç¬¬ 7-8 è¡Œ)
```python
# âŒ é¡¶å±‚å¯¼å…¥ deployment å­æ¨¡å—
from .modal.deployer import ModalDeployer
from .triton.provider import TritonProvider
```

**é—®é¢˜**: è¿™äº› deployer åªåœ¨éƒ¨ç½²åœºæ™¯ä½¿ç”¨ï¼Œä¸åº”åœ¨æ¨¡å—åˆå§‹åŒ–æ—¶å¯¼å…¥ã€‚

**3. `isa_model/deployment/modal/deployer.py`** (ç¬¬ 16 è¡Œ)
```python
from huggingface_hub import HfApi, model_info
```

**4. `isa_model/deployment/triton/provider.py`** (ç¬¬ 16 è¡Œ)
```python
import docker
```

## å½±å“èŒƒå›´

### å½“å‰ workaround
åœ¨ `pyproject.toml` çš„ `[cloud]` extras ä¸­æ·»åŠ äº†è¿™äº›ä¾èµ–ï¼š
```toml
cloud = [
    ...
    "huggingface-hub>=0.16.0",  # æœ¬åº”åœ¨ [local]
    "docker>=6.0.0",  # æœ¬åº”åœ¨ k8s/deployment extras
]
```

### åŒ…å¤§å°å½±å“
- `huggingface-hub`: ~300KB
- `docker`: ~600KB
- **æ€»é¢å¤–è´Ÿæ‹…**: ~900KB + ä¾èµ–é“¾

### æ­£ç¡®çš„ä¾èµ–åˆ†é…åº”è¯¥æ˜¯

| åŒ… | åº”è¯¥åœ¨çš„ extras | å½“å‰åœ¨çš„ extras | å®é™…ç”¨é€” |
|---|---|---|---|
| `huggingface-hub` | `[local]` | `[cloud]` âœ… (workaround) | æœ¬åœ°æ¨¡å‹ä¸‹è½½ |
| `docker` | `[k8s]` æˆ–å•ç‹¬ä½¿ç”¨ | `[cloud]` âœ… (workaround) | Triton å®¹å™¨éƒ¨ç½² |

## å»ºè®®çš„ä¿®å¤æ–¹æ¡ˆ

### æ–¹æ¡ˆ A: å»¶è¿Ÿå¯¼å…¥ï¼ˆæ¨èï¼‰

**ä¿®æ”¹ `isa_model/inference/services/llm/__init__.py`**:
```python
"""
LLM Services - Business logic services for Language Models
"""

# Cloud-mode services (always available)
from .ollama_llm_service import OllamaLLMService
from .openai_llm_service import OpenAILLMService
from .yyds_llm_service import YydsLLMService
from .huggingface_llm_service import ISALLMService, HuggingFaceLLMService, HuggingFaceInferenceService

# Lazy import for local-mode services (avoid loading heavy deps)
# Only import when actually used:
#   from isa_model.inference.services.llm.local_llm_service import LocalLLMService

__all__ = [
    "OllamaLLMService",
    "OpenAILLMService", 
    "YydsLLMService",
    "ISALLMService",
    "HuggingFaceLLMService", 
    "HuggingFaceInferenceService",
    # "LocalLLMService",  # Available via explicit import
]
```

**ä¿®æ”¹ `isa_model/deployment/__init__.py`**:
```python
"""
ISA Model Deployment - Multi-provider deployment system

Unified deployment architecture supporting Modal and Triton platforms.
"""

# Lazy imports - only load when explicitly used:
#   from isa_model.deployment.modal.deployer import ModalDeployer
#   from isa_model.deployment.triton.provider import TritonProvider
#   from isa_model.deployment.core.deployment_manager import DeploymentManager

__all__ = ["ModalDeployer", "TritonProvider", "DeploymentManager"]
```

### æ–¹æ¡ˆ B: æ¡ä»¶å¯¼å…¥

```python
# isa_model/inference/services/llm/__init__.py
import sys

# ... å…¶ä»–å¯¼å…¥ ...

# åªåœ¨ local extras å¯ç”¨æ—¶å¯¼å…¥
if "torch" in sys.modules or "transformers" in sys.modules:
    try:
        from .local_llm_service import LocalLLMService, create_local_llm_service
        __all__.extend(["LocalLLMService", "create_local_llm_service"])
    except ImportError:
        pass
```

### æ–¹æ¡ˆ C: æ‹†åˆ†å­åŒ…ï¼ˆé•¿æœŸæ–¹æ¡ˆï¼‰

å°† `isa_model` æ‹†åˆ†ä¸ºå¤šä¸ªå¯é€‰å®‰è£…çš„å­åŒ…ï¼š
- `isa-model-core` - æ ¸å¿ƒ API
- `isa-model-cloud` - äº‘æœåŠ¡é›†æˆ
- `isa-model-local` - æœ¬åœ°æ¨ç†
- `isa-model-deploy` - éƒ¨ç½²å·¥å…·

## æµ‹è¯•éªŒè¯

ä¿®å¤ååº”éªŒè¯ï¼š

```bash
# 1. Cloud æ¨¡å¼ä¸åº”å¯¼å…¥æœ¬åœ°ä¾èµ–
docker run --rm isa-mcp:test python -c "
import sys
import isa_model
print('Loaded modules:', [m for m in sys.modules if 'torch' in m or 'docker' in m])
# åº”è¯¥è¾“å‡º: Loaded modules: ['docker'] (docker ç”¨äºéƒ¨ç½²)
# ä¸åº”è¯¥åŒ…å«: torch, transformers
"

# 2. åŒ…å¤§å°æ£€æŸ¥
pip install isa_model[cloud]
pip list --format=freeze | grep -E "docker|huggingface|torch"
# åº”è¯¥åªæœ‰äº‘ API ç›¸å…³ä¾èµ–
```

## å®æ–½ä¼˜å…ˆçº§

- **P0 (é«˜ä¼˜å…ˆçº§)**: ä¿®å¤ `deployment/__init__.py` - é¿å…å¯¼å…¥ deployer æ¨¡å—
- **P1 (ä¸­ä¼˜å…ˆçº§)**: ä¿®å¤ `services/llm/__init__.py` - LocalLLMService å»¶è¿Ÿå¯¼å…¥
- **P2 (ä½ä¼˜å…ˆçº§)**: æ‹†åˆ†å­åŒ…ç»“æ„

## ä¸´æ—¶ Workaround

å½“å‰å·²åœ¨ `pyproject.toml:54` æ·»åŠ  TODO æ³¨é‡Šï¼š
```toml
"docker>=6.0.0",  # TODO: Should not be needed for cloud mode, fix import structure (see issue3.md)
```

## ç›¸å…³æ–‡ä»¶

- `/Users/xenodennis/Documents/Fun/isA_Model/isa_model/inference/services/llm/__init__.py:10`
- `/Users/xenodennis/Documents/Fun/isA_Model/isa_model/deployment/__init__.py:7-8`
- `/Users/xenodennis/Documents/Fun/isA_Model/isa_model/deployment/modal/deployer.py:16`
- `/Users/xenodennis/Documents/Fun/isA_Model/isa_model/deployment/triton/provider.py:16`
- `/Users/xenodennis/Documents/Fun/isA_Model/pyproject.toml:47-55`

## çŠ¶æ€

ğŸŸ¡ **WORKAROUND IN PLACE** - å·²æ·»åŠ ä¾èµ–åˆ° `[cloud]` extrasï¼Œç­‰å¾…é‡æ„ä¿®å¤

## ç›¸å…³ Issues

- Issue #1: "No module named 'isa_model'" - å¼•å‘æ­¤é—®é¢˜çš„è°ƒæŸ¥
